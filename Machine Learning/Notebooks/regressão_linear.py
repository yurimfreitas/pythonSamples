# -*- coding: utf-8 -*-
"""Regressão Linear.ipynb

Automatically generated by Colab.

# Modelo de regressão linear

---


[documentação](https://scikit-learn.org/stable/index.html) <br>
dataset: [fonte](https://www.kaggle.com/datasets/hellbuoy/car-price-prediction?select=CarPrice_Assignment.csv)

---

>[Modelo de regressão linear](#scrollTo=QoBv84MIUa-h)

>>[1. Vamos começar por importar os packages e o dataset](#scrollTo=HO6jdFDmldaU)

>>[2. Vamos explorar o dataset](#scrollTo=2S0UrVDEf8E-)

>>[3. Vamos visualizar a informação](#scrollTo=lYnaR0LYO-Sg)

>>[4. Vamos aplicar o modelo de regressão](#scrollTo=BKzodGb8R7t9)

>>>[4.1. Primeira abordagem (1 variável characterística)](#scrollTo=6AUIuQaWSCdM)

>>>[4.2. Vamos considerar as várias características disponíveis](#scrollTo=VlyljBsUgD8D)

>>[5. Vamos agora aplicar a regressão com regularização](#scrollTo=Z-_Mfy5StNhM)

>>>[5.1. Ridge](#scrollTo=FxZ0QYaAfu1S)

>>>[5.2. Lasso](#scrollTo=bsFM5l28hFLs)

>>[6. Vamos visualizar os resíduos](#scrollTo=bJoXSNz21wUm)

## 1.&nbsp;Vamos começar por importar os packages e o dataset
"""

# packages gerais
import pandas as pd
import numpy as np

# dataset
df_car_price = pd.read_csv("CarPrice_Assignment.csv")

"""## 2.&nbsp;Vamos explorar o dataset"""

# exploração inicial
df_car_price.head()
# df_car_price.info()
# df_car_price.shape
# df_car_price.describe()

"""## 3.&nbsp;Vamos visualizar a informação

"""

# importamos o matplotlib.pyplot
import matplotlib.pyplot as plt
plt.style.use('ggplot')
#definimos as variáveis que queremos visualizar (enginesize e price)
x_variable = df_car_price["enginesize"]
y_variable = df_car_price["price"]
plt.scatter(x_variable, y_variable, color = 'b')
plt.xlim(0, x_variable.max()+10)
plt.ylim(0, y_variable.max()+1000)
plt.ylabel("preço do veículo")
plt.xlabel('dimensão do motor')
plt.show()

"""## 4.&nbsp;Vamos aplicar o modelo de regressão"""

# definimos a variável alvo
target_variable = "price"

# train_test split usando a função train_test_split
# -> não consideramos stratification

X = df_car_price.drop([target_variable], axis = 1)
y = df_car_price[target_variable]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size = 0.3,
                                                    random_state = 12)

"""### 4.1.&nbsp;Primeira abordagem (1 variável characterística)"""

# vamos escolher enginesize como variável independente
X_1_feature = X_train[['enginesize']]

# vamos importar o modelo
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X_1_feature, y_train)

# vamos prever para todos os valores
y_reg = lin_reg.predict(X[['enginesize']].sort_values('enginesize'))

# podemos ver os coeficientes da regressão
m = lin_reg.coef_[0]
b = lin_reg.intercept_
print("m: {}; b: {}".format(m , b))

# para verificar a accuracy do modelo aplicamos a função .score (#R2)
print("r2: {}".format(lin_reg.score(X_test[['enginesize']], y_test).round(3)))

# vamos visualizar o resultado
plt.scatter(X_train[['enginesize']], y_train, color = 'k')
plt.plot(X[['enginesize']].sort_values('enginesize'), y_reg, color = 'r', linewidth = 2)
plt.xlim(0, x_variable.max()+10)
plt.ylim(0, y_variable.max()+1000)
plt.ylabel("preço do veículo")
plt.xlabel('dimensão do motor')
plt.show()

# vamos visualizar o resultado
plt.scatter(X_test[['enginesize']], y_test, color = 'b')
plt.plot(X[['enginesize']].sort_values('enginesize'), y_reg, color = 'r', linewidth = 2)
plt.xlim(0, x_variable.max()+10)
plt.ylim(0, y_variable.max()+1000)
plt.ylabel("preço do veículo")
plt.xlabel('dimensão do motor')
plt.show()

"""### 4.2.&nbsp;Vamos considerar as várias características disponíveis"""

# importamos o modelo
from sklearn.linear_model import LinearRegression
lin_reg_all = LinearRegression()
lin_reg_all.fit(X_train, y_train)
y_pred = lin_reg_all.predict(X_test)

# podemos ver os coeficientes da regressão
betas = lin_reg_all.coef_
beta0 = lin_reg_all.intercept_
print("betas: {}; beta0: {}".format(betas , beta0))

# para verificar a accuracy do modelo aplicamos a função .score (#R2)
print("r2: {}".format(lin_reg_all.score(X_test, y_test).round(3)))

"""## 5.&nbsp;Vamos agora aplicar a regressão com regularização

### 5.1.&nbsp;Ridge
"""

# importamos o modelo
from sklearn.linear_model import Ridge
ridge_reg = Ridge(alpha = 0.3)
ridge_reg.fit(X_train, y_train)
y_pred_ridge = ridge_reg.predict(X_test)

# podemos ver os coeficientes da regressão
betas = ridge_reg.coef_
beta0 = ridge_reg.intercept_
print("betas: {}; beta0: {}".format(betas , beta0))

# para verificar a accuracy do modelo aplicamos a função .score (#R2)
print("r2: {}".format(ridge_reg.score(X_test, y_test).round(3)))

"""### 5.2.&nbsp;Lasso"""

# importamos o modelo
from sklearn.linear_model import Lasso
lasso_reg = Lasso(alpha = 2)
lasso_reg.fit(X_train, y_train)
y_pred_lasso = lasso_reg.predict(X_test)

# podemos ver os coeficientes da regressão
betas = lasso_reg.coef_
beta0 = lasso_reg.intercept_
print("betas: {}; beta0: {}".format(betas , beta0))

# para verificar a accuracy do modelo aplicamos a função .score (#R2)
print("r2: {}".format(lasso_reg.score(X_test, y_test).round(3)))

"""## 6.&nbsp;Vamos visualizar os resíduos"""

from sklearn.metrics import PredictionErrorDisplay

fig, axs = plt.subplots(nrows=2, figsize=(4, 8))
PredictionErrorDisplay.from_predictions(
    y_true = y_test,
    y_pred = y_pred_lasso,
    kind="actual_vs_predicted",
    ax=axs[0],
)
axs[0].set_title("Reais vs. Previstos")
axs[0].set_xlabel("Valores Previstos")
axs[0].set_ylabel("Valores Reais")

PredictionErrorDisplay.from_predictions(
    y_true = y_test,
    y_pred=y_pred_lasso,
    kind="residual_vs_predicted",
    ax=axs[1],
)

axs[1].set_title("Resíduos vs. Valores Previstos")
axs[1].set_xlabel("Valores Previstos")
axs[1].set_ylabel("Resíduos")

plt.tight_layout()
plt.show()